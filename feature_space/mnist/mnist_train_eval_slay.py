# -*- coding: utf-8 -*-
"""mnist_train_eval_slay

Automatically generated by Colaboratory.
"""

import numpy as np
import tensorflow.compat.v2 as tf
from sklearn.model_selection import train_test_split
import time

tf.enable_v2_behavior()
# tf.compat.v1.flags.DEFINE_string('f', '', 'kernel')

from pllay import *

"""# Training and Evaluation for SLay"""

nmax_diag = 32


class MNIST_MLP_SLay(tf.keras.Model):
    def __init__(self, name='mnistmlpslay', unitsDense=64, unitsTop=10, **kwargs):
        super(MNIST_MLP_SLay, self).__init__(name=name, **kwargs)
        # self.layer1 = tf.keras.layers.Dense(32, name='dense_1')
        # self.layer1 = TopoWeightLayer(32, m0=0.1, tseq=[v/10. for v in range(31)], KK=list(range(30)), by=1./13.5)
        # self.layer1_1 = TopoWeightLayer(unitsTop, m0=0.05, tseq=np.linspace(0.06, 0.3, 25), KK=list(range(2)), by=1./13.5)
        # self.layer1_2 = TopoWeightLayer(unitsTop, m0=0.2, tseq=np.linspace(0.14, 0.4, 27), KK=list(range(3)), by=1./13.5)
        self.layer1_1 = HoferLayer(unitsTop, nu=0.01, name='hofer')
        self.layer1_2 = HoferLayer(unitsTop, nu=0.005, name='hofer')
        # self.layer2 = tf.keras.layers.Dense(32, activation='relu', name='dense_2') 
        self.layer2 = tf.keras.layers.Dense(unitsDense, activation='relu', name='dense_2') 
        self.layer3 = tf.keras.layers.Dense(10, name='predictions')

    def call(self, x):
        xg, xl1, xl2, xd1, xd2 = tf.split(x, [784, 100, 162, 4*nmax_diag, 4*nmax_diag], axis=-1)
        xd1 = tf.reshape(xd1, [16, 2*nmax_diag, 2])
        xd2 = tf.reshape(xd2, [16, 2*nmax_diag, 2])
        xd1 =  tf.nn.relu(self.layer1_1(xd1))
        xd2 =  tf.nn.relu(self.layer1_2(xd2))
        x = tf.concat((xg, xd1, xd2), -1)
        # x = xg
        x = self.layer2(x)
        x = self.layer3(x)
        print(x.shape)
        return x


class MNIST_CNN_SLay(tf.keras.Model):
    def __init__(self, name='mnistcnnslay', filters=32, kernel_size=3, unitsDense=64, unitsTopInput=10, **kwargs):
        super(MNIST_CNN_SLay, self).__init__(name=name, **kwargs)
        # self.layer1_1 = tf.keras.layers.Conv2D(32, 3, padding="same", activation='relu')
        # self.layer1_2 = tf.keras.layers.Conv2D(1, 3, padding="same", activation='relu')
        self.layer1_1 = tf.keras.layers.Conv2D(filters, kernel_size, padding="same", activation='relu')
        self.layer1_2 = tf.keras.layers.Conv2D(1, kernel_size, padding="same", activation='relu')
        # self.layer1_3 = TopoWeightLayer(32, m0=0.1, tseq=[v/10. for v in range(31)], KK=list(range(30)), by=1./13.5)
        # self.layer1_3 = TopoWeightLayer(unitsTopMiddle, m0=0.05, tseq=np.linspace(0.06, 0.3, 25), KK=list(range(2)), by=1./13.5)
        # self.layer1_4 = TopoWeightLayer(unitsTopMiddle, m0=0.2, tseq=np.linspace(0.14, 0.4, 27), KK=list(range(3)), by=1./13.5)
        # self.layer1_3 = TopoFunLayer(unitsTopMiddle, grid_size=[28, 28], tseq=np.linspace(0.05, 0.95, 18), KK=list(range(3)))
        # self.layer2_1 = TopoWeightLayer(unitsTopInput, m0=0.05, tseq=np.linspace(0.06, 0.3, 25), KK=list(range(2)), by=1./13.5)
        # self.layer2_2 = TopoWeightLayer(unitsTopInput, m0=0.2, tseq=np.linspace(0.14, 0.4, 27), KK=list(range(3)), by=1./13.5)
        self.layer2_1 = HoferLayer(unitsTopInput, nu=0.01, name='hofer')
        self.layer2_2 = HoferLayer(unitsTopInput, nu=0.005, name='hofer')
        # self.layer3 = tf.keras.layers.Dense(32, activation='relu', name='dense_2') 
        self.layer3 = tf.keras.layers.Dense(unitsDense, activation='relu', name='dense_2') 
        self.layer4 = tf.keras.layers.Dense(10, name='predictions')

    def call(self, x):
        xg, xl1, xl2, xd1, xd2 = tf.split(x, [784, 100, 162, 4*nmax_diag, 4*nmax_diag], axis=-1)
        xd1 = tf.reshape(xd1, [16, 2*nmax_diag, 2])
        xd2 = tf.reshape(xd2, [16, 2*nmax_diag, 2])
        xg = tf.reshape(xg, [16, 28, 28, 1])
        xg1 = self.layer1_1(xg)
        xg1 = self.layer1_2(xg1)
        #print(x1.shape)
        xg1 = tf.reshape(xg1, [16, 784])
        # xg1_1 = tf.nn.relu(self.layer1_3(xg1))
        # xg1_2 = tf.nn.relu(self.layer1_4(xg1))
        # xg1 = tf.concat((xg1, xg1_1, xg1_2), -1)
        # xg1 = tf.concat((xg1, xg1_1), -1)
        xd1 = tf.nn.relu(self.layer2_1(xd1))
        xd2 = tf.nn.relu(self.layer2_2(xd2))
        x = tf.concat((xg1, xd1, xd2), -1)
        # x = xg1
        x = self.layer3(x)
        x = self.layer4(x)
        print(x.shape)
        return x

def mnist_train_eval_slay(nTimes, epochs, es, rand_state, corrupt_prob_list,
      noise_prob_list, x_processed_file_list, y_file, batch_size=16):

    print("epochs = ", epochs)
    print("nTimes = ", nTimes)
    print("random states = ", rand_state)

    accuracy_mlp_slay = np.zeros(nTimes)
    accuracy_cnn_slay = np.zeros(nTimes)

    (y_train, y_test) = np.load(y_file, allow_pickle=True)

    for iCn in range(nCn):
        start_time = time.time()
        print("--------------------------------------------------------------")
        print("Corruption rate = ", corrupt_prob_list[iCn])
        print("Noise rate = ", noise_prob_list[iCn])
        print("--------------------------------------------------------------") 
        (x_train_processed, x_test_processed) = np.load(
              x_processed_file_list[iCn], allow_pickle=True)
        test_dataset = to_tf_dataset(x=x_test_processed, y=y_test,
              batch_size=batch_size)

        for iTime in range(nTimes):

            # Train validate split
            x_train_split, x_val, y_train_split, y_val = train_test_split(
                  x_train_processed, y_train, test_size=0.1,
                  random_state=rand_state[iTime], shuffle=True,
                  stratify=y_train)
            train_dataset = to_tf_dataset(x=x_train_split, y=y_train_split,
                  batch_size=batch_size)
            val_dataset = to_tf_dataset(x=x_val, y=y_val,
                  batch_size=batch_size)

            # MLP + SLay
            start_time_inside = time.time()
            np.random.seed(rand_state[iTime])
            print("MLP + SLay")
            model_mlp_slay = MNIST_MLP_SLay()
            model_mlp_slay.compile(optimizer=tf.keras.optimizers.RMSprop(),  # Optimizer
                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                  metrics=['sparse_categorical_accuracy'])
            hist_mlp_slay = model_mlp_slay.fit(train_dataset, epochs=epochs,
                  callbacks=[es], validation_data=val_dataset)
            accuracy_mlp_slay[iTime] = model_mlp_slay.evaluate(test_dataset)[1]
            print("--- %s seconds ---" % (time.time() - start_time_inside))

            # CNN + SLay
            start_time_inside = time.time()
            np.random.seed(rand_state[iTime])
            print("CNN + SLay")
            model_cnn_slay = MNIST_CNN_SLay()
            model_cnn_slay.compile(optimizer=tf.keras.optimizers.RMSprop(),  # Optimizer
                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                  metrics=['sparse_categorical_accuracy'])
            hist_cnn_slay = model_cnn_slay.fit(train_dataset, epochs=epochs,
                  callbacks=[es], validation_data=val_dataset)
            accuracy_cnn_slay[iTime] = model_cnn_slay.evaluate(test_dataset)[1]
            # model_cnn_slay.save(model_cnn_slay_file_array[iCn][iTime])
            print("--- %s seconds ---" % (time.time() - start_time_inside))

        print("--------------------------------------------------------------")
        print("Corruption rate = ", corrupt_prob_list[iCn])
        print("Noise rate = ", noise_prob_list[iCn])
        print("--------------------------------------------------------------")  
        print("Accuracy for MLP + SLay :", accuracy_mlp_slay)
        print("Average Accuracy for MLP + SLay :",
              sum(accuracy_mlp_slay) / nTimes)
        print("Accuracy for CNN + SLay :", accuracy_cnn_slay)
        print("Average Accuracy for CNN + SLay :",
              sum(accuracy_cnn_slay) / nTimes)

        print("--- %s seconds ---" % (time.time() - start_time))
        print("--------------------------------------------------------------")

# corrupt_prob_list = [0.1]
# noise_prob_list = [0.1]
corrupt_prob_list = [0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35]
noise_prob_list = [0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35]
nCn = len(corrupt_prob_list)
file_cn_list = [None] * nCn
for iCn in range(nCn):
    file_cn_list[iCn] = str(int(corrupt_prob_list[iCn] * 100)).zfill(2) + \
          '_' + str(int(noise_prob_list[iCn] * 100)).zfill(2)

x_processed_file_list = [None] * nCn
for iCn in range(nCn):
    x_processed_file_list[iCn] = (
          'mnist_x_processed_' + file_cn_list[iCn] + '.npy')
y_file = 'mnist_y.npy'

# nTimes=1
nTimes=10

batch_size = 16

# epochs=10
epochs=100
es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.003,
      patience=3, verbose=1)

np.random.seed(0)
rand_state = [np.random.randint(2**16) for iTimes in range(nTimes)]
# rand_state = (
#       [2732, 43567, 42613, 52416, 45891, 21243, 30403, 32103, 41993, 57043])

mnist_train_eval_slay(nTimes=nTimes, epochs=epochs, es=es,
      rand_state=rand_state, corrupt_prob_list=corrupt_prob_list,
      noise_prob_list=noise_prob_list,
      x_processed_file_list=x_processed_file_list, y_file=y_file,
      batch_size=batch_size)